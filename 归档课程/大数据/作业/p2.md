##### 211275022田昊东

#### 1.简述为什么需要并行计算？

- **传统提升计算机性能的手段**几乎**耗尽**，单核处理器性能提升**接近极限**。为了进一步提高计算速度，向多核并行计算发展成为发展的必然趋势。
- 涉及到大规模数据和复杂计算的研究及应用领域**需要使用并行计算技术**，并且金融、医疗、ai 等越来越多的领域**对大数据处理具有很高的需求**。
- 并行计算技术将对传统计算技术产生革命性的影响, 很多传统的串行算法和计算方法都将需要重新研究和设计其并行化算法和计算方法.

#### 2.并行计算按照系统类型划分，可以分为哪几种？简述每一种系统类型的特点。

- 多核/众核并行计算系统MC：集成在一个芯片上，使用本地缓存加上全局内存，功耗较低。
- 对称多处理系统SMP：多个相同类型的树立起通过总线连接并使用共享内存，扩展性差，规模较小。
- 大规模并行处理MPP：使用独立处理器，具有独立的内存，通过专用内联网连接，扩展性差，规模中等。
- 集群Cluster：使用商品化服务器，通过网络互联，可扩展性强，目前最为常用。
- 网格Grid：有距离较远的异构服务器构成，较为松散，适合并行度较低的大规模科学计算。
- 云Cloud：通过互联网访问计算资源，资源托管在云服务提供商的远程数据中心

#### 3.为什么可靠性设计与容错是并行计算的主要技术问题？

- 使用并行计算需要**消耗大量计算资源**，并且由于系统设备众多、较为复杂，单个节点的出错和失效不可避免，此如果一个节点的**错误会导致整个计算的崩溃**，那会导致系统频繁发生故障，**计算根本无法进行**。
- 并行计算常常用于科学计算、电影渲染等需要**长时间持续计算**的领域，如**频繁因一点小故障而终止**那么计算可能根本无法完成
- 并行计算的结果往往是具有**重要价值**的，错误的结果会带来高昂的代价
- 增加可靠性，避免出错还可以介绍系统**维护成本**，是提高并行计算竞争力的关键

#### 4.MPI提供哪几种通信方式？简述每种通信方式对应的接口。

- 点对点通信
  - 同步通信
    - 发送：`MPI_Send (buf, count, datatype, dest, tag, comm)`
    - 接收：`MPI_Recv (buf, count, datatype, source, tag, comm, status)`

  - 异步通信
    - 发送：`MPI_ISend (buf, count, datatype, dest, tag, comm, request)`
    - 接收：`MPI_IRecv (buf, count, datatype, source, tag, comm, status, request)`
    - 等待传输完成：`MPI_Wait (request, status)`
    - 检查是否传输完成`MPI_Test (request, flag, status)`

- 节点集合通信
  - 同步（设置同步障碍，使所有进程的执行同时完成，即等待所有进程达到同步点后才能继续执行。）：`int MPI_Barrier(MPI_Comm comm)`
  - 广播发送：`int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)`
  - 收集（多个进程的消息以某种次序收集到一个进程）：`int MPI_Gather(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)`
  - 分散（将一个信息划分为等长的段依次发送给其他进程）：`int MPI_Scatter(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)`
  - 数据规约（将一组进程的数据按照指定的操作方式规约到一起，并传送给一个进程）：`MPI_Reduce(sendbuf, recvbuf, count, datatype, op, root, comm)`

- 用户自定义的复合数据类型传输

#### 5.尝试在单机上安装并运行MPICH，并运行讲义中计算积分（P72-74）的简单示例。

- 使用ubuntu20.4虚拟机
- <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918181810245.png" alt="image-20230918181810245" style="zoom:33%;" />