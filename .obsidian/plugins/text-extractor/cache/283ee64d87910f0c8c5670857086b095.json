{"path":"docs/å­¦æ ¡è¯¾ç¨‹/å½’æ¡£è¯¾ç¨‹/å¤§æ•°æ®/è¯¾ä»¶/06 MapReduce Basic Programming (I).pdf","text":"MapReduceåŸºç¡€ç®—æ³•ç¨‹åºè®¾è®¡ (I) æ‘˜è¦ p MapReduceå¯è§£å†³å“ªäº›ç®—æ³•é—®é¢˜ï¼Ÿ p å›é¡¾ï¼šMapReduceæµæ°´çº¿ p MapReduce WordCount1.0 p MapReduce WordCount2.0 p MapReduceçŸ©é˜µä¹˜æ³• p MapReduceå…³ç³»ä»£æ•°è¿ç®— 2 åº”ç”¨èŒƒå›´ Â¨ è‡ªMapReduceå‘æ˜åï¼ŒGoogleå¤§é‡ç”¨äºå„ç§æµ·é‡æ•°æ®å¤„ç†ã€‚ç›®å‰Googleå†…éƒ¨æœ‰7åƒä»¥ä¸Šçš„ç¨‹åº åŸºäºMapReduceå®ç°ã€‚MapReduceå¯å¹¿æ³›åº”ç”¨äºæœç´¢å¼•æ“ï¼ˆæ–‡æ¡£å€’æ’ç´¢å¼•ï¼Œç½‘é¡µé“¾æ¥å›¾åˆ†æä¸ é¡µé¢æ’åºç­‰ï¼‰ã€Webæ—¥å¿—åˆ†æã€æ–‡æ¡£åˆ†æå¤„ç†ã€æœºå™¨å­¦ä¹ ã€æœºå™¨ç¿»è¯‘ç­‰å„ç§å¤§è§„æ¨¡æ•°æ®å¹¶è¡Œ è®¡ç®—åº”ç”¨é¢†åŸŸå„ç±»å¤§è§„æ¨¡æ•°æ®å¹¶è¡Œå¤„ç†ç®—æ³•ã€‚ 3 åŸºæœ¬ç®—æ³• å„ç§å…¨å±€æ•°æ®ç›¸å…³æ€§å°ã€èƒ½é€‚å½“åˆ’åˆ†æ•°æ®çš„è®¡ç®—ä»»åŠ¡ï¼Œå¦‚ï¼š Â¨ åˆ†å¸ƒå¼æ’åº Â¨ åˆ†å¸ƒå¼GREP(æ–‡æœ¬åŒ¹é…æŸ¥æ‰¾) Â¨ å…³ç³»ä»£æ•°æ“ä½œ å¦‚ï¼šé€‰æ‹©ï¼ŒæŠ•å½±ï¼Œæ±‚äº¤é›†ã€å¹¶é›†ï¼Œè¿æ¥ï¼Œæˆç»„ï¼Œèšåˆâ€¦ Â¨ çŸ©é˜µå‘é‡ç›¸ä¹˜ã€çŸ©é˜µç›¸ä¹˜ Â¨ è¯é¢‘ç»Ÿè®¡(word count)ï¼Œè¯é¢‘é‡è¦æ€§åˆ†æ(TF-IDF) Â¨ å•è¯åŒç°å…³ç³»åˆ†æ å…¸å‹çš„åº”ç”¨å¦‚ä»ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸­è‡ªåŠ¨æŒ–æ˜åŸºå› äº¤äº’ä½œç”¨å…³ç³» Â¨ æ–‡æ¡£å€’æ’ç´¢å¼• Â¨ â€¦â€¦ 4 å¤æ‚ç®—æ³•åŠåº”ç”¨ Â— Webæœç´¢å¼•æ“ Â¤ ç½‘é¡µçˆ¬å–ã€å€’æ’ç´¢å¼•ã€ç½‘é¡µæ’åºã€æœç´¢ç®—æ³• Â— Webè®¿é—®æ—¥å¿—åˆ†æ Â¤ åˆ†æå’ŒæŒ–æ˜ç”¨æˆ·åœ¨Webä¸Šçš„è®¿é—®ã€è´­ç‰©è¡Œä¸ºç‰¹å¾ã€ä»¥å®šåˆ¶ä¸ªæ€§åŒ–ç”¨æˆ·ç•Œé¢æˆ–æŠ•æ”¾ç”¨æˆ·æ„Ÿå…´è¶£çš„äº§å“å¹¿ å‘Š Â— æ•°æ®/æ–‡æœ¬ç»Ÿè®¡åˆ†æ Â¤ å¦‚ç§‘æŠ€æ–‡çŒ®å¼•ç”¨å…³ç³»åˆ†æå’Œç»Ÿè®¡ã€ä¸“åˆ©æ–‡çŒ®å¼•ç”¨åˆ†æå’Œç»Ÿè®¡ Â— å›¾ç®—æ³• Â¤ å¹¶è¡ŒåŒ–å®½åº¦ä¼˜å…ˆæœç´¢(æœ€çŸ­è·¯å¾„é—®é¢˜ï¼Œå¯å…‹æœDijkstraä¸²è¡Œç®—æ³•çš„ä¸è¶³)ï¼Œæœ€å°ç”Ÿæˆæ ‘ï¼Œå­æ ‘æœç´¢ã€æ¯”å¯¹ Â¤ Webé“¾æ¥å›¾åˆ†æç®—æ³•PageRankï¼Œåƒåœ¾é‚®ä»¶è¿æ¥åˆ†æ Â— èšç±»(clustering) Â¤ æ–‡æ¡£èšç±»ã€å›¾èšç±»ã€å…¶å®ƒæ•°æ®é›†èšç±» 5 å¤æ‚ç®—æ³•åŠåº”ç”¨ Â— ç›¸ä¼¼æ€§æ¯”è¾ƒåˆ†æç®—æ³• Â¤ å­—ç¬¦åºåˆ—ã€æ–‡æ¡£ã€å›¾ã€æ•°æ®é›†ç›¸ä¼¼æ€§æ¯”è¾ƒåˆ†æ Â— åŸºäºç»Ÿè®¡çš„æ–‡æœ¬å¤„ç† Â¤ æœ€å¤§æœŸæœ›(EM)ç»Ÿè®¡æ¨¡å‹ï¼Œéšé©¬å¯å¤«æ¨¡å‹(HMM)ï¼Œâ€¦â€¦ Â— æœºå™¨å­¦ä¹  Â¤ ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€åˆ†ç±»ç®—æ³•(å†³ç­–æ ‘ã€SVMâ€¦) Â— æ•°æ®æŒ–æ˜ Â— ç»Ÿè®¡æœºå™¨ç¿»è¯‘ Â— ç”Ÿç‰©ä¿¡æ¯å¤„ç† Â¤ DNAåºåˆ—åˆ†ææ¯”å¯¹ç®—æ³•Blastï¼šåŒåºåˆ—æ¯”å¯¹ã€å¤šåºåˆ—æ¯”å¯¹ Â¤ ç”Ÿç‰©ç½‘ç»œåŠŸèƒ½æ¨¡å—(Motif)æŸ¥æ‰¾å’Œæ¯”å¯¹ Â— å¹¿å‘Šæ¨é€ä¸æ¨èç³»ç»Ÿ Â— â€¦â€¦ 6 MapReduceç®—æ³•åº”ç”¨ä¸“è‘— 1.Mining of Massive Datasets 2010, Jure Leskovec (Stanford Univ.), Anand Rajaraman (Kosmix, Inc), Jeffrey D. Ullman (Stanford Univ.) ä¸»è¦ä»‹ç»åŸºäºMapReduceçš„å¤§è§„æ¨¡æ•°æ®æŒ–æ˜ç›¸å…³çš„æŠ€æœ¯å’Œç®—æ³•ï¼Œå°¤å…¶æ˜¯Webæˆ–è€…ä»Webå¯¼å‡ºçš„ æ•°æ® 7 Ch3. Similarity search, including the key techniques of minhashing and locality- sensitive hashing. Ch4. Data-stream processing and specialized algorithms for dealing with data that arrives so fast it must be processed immediately or lost. Ch5. The technology of search engines, including Googleâ€™s PageRank, link-spam detection, and the hubs-and-authorities approach(a link analysis algorithmï¼š Hyperlink-Induced Topic Search (HITS)). Ch6. Frequent-itemset mining, including association rules, market-baskets, the A-Priori Algorithm and its improvements (a classic algorithm for learning association rules). Ch7. Algorithms for clustering very large, high-dimensional datasets. Ch8. Two key problems for Web applications: managing advertising and recommendation systems. MapReduceç®—æ³•åº”ç”¨ä¸“è‘— 2. Data-Intensive Text Processing with MapReduce Jimmy Lin and Chris Dyerï¼Œ2010ï¼ŒUniversity of Maryland, College Park ä¸»è¦ä»‹ç»åŸºäºMapReduceçš„å¤§è§„æ¨¡æ–‡æ¡£æ•°æ®å¤„ç†æŠ€æœ¯å’Œç®—æ³• 8 Ch4. Inverted Indexing for Text Retrieval Ch5. Graph Algorithms Parallel Breadth-First Search PageRank Ch6. EM Algorithms for Text Processing EM, HMM Case Study: Word Alignment for Statistical Machine Translation å›é¡¾ï¼šMapReduceæµæ°´çº¿ MapReduce Pipeline map(K1,V1)->[(K2,V2)] shuffle and sort reduce(K2,[V2]) -> [(K3,V3)] ([â€¦] denotes a list ) Any algorithm that you wish to develop must be expressed in terms of such rigidly-defined components 9 å›é¡¾ï¼šMapReduceæµæ°´çº¿ Â¨ Mapper Â¤ Initialize: setup() Â¤ map(): It is called once for each key/value pair in the input split. The default is the identity function. Â¤ Close: cleanup() Â¨ Shuffle Â¤ Shuffle phase needs the Partitioner to route the output of mapper to reducer. Â¤ Partitioner controls the partitioning of the keys of the intermediate map-outputs. The key is used to derive the partition, typically by a hash function. The total number of partitions is the same as the number of reduce tasks for the job. Â¤ HashPartitioner is the default Partitioner. 10 å›é¡¾ï¼šMapReduceæµæ°´çº¿ Â¨ Sort Â¤ We can controls how the keys are sorted before they are passed to the Reducer by using a customized comparator. Â¨ Reducer Â¤ Initialize: setup() Â¤ reduce(): It is called once for each key. The default implementation is an identity function. Â¤ Close: cleanup() 11 å¸¸ç”¨æ•°æ®ç±»å‹ Â¨ è¿™äº›æ•°æ®ç±»å‹éƒ½å®ç°äº†WritableComparableæ¥å£ï¼Œä»¥ä¾¿è¿›è¡Œç½‘ç»œä¼ è¾“å’Œæ–‡ä»¶å­˜ å‚¨ï¼Œä»¥åŠè¿›è¡Œå¤§å°æ¯”è¾ƒã€‚ 12 Hadoop APIæ–‡æ¡£ Â¨ Apache Hadoop Main 3.3.6 API Â¤ Common; HDFS; MapReduce; YARN Â¤ https://hadoop.apache.org/docs/stable/api/index.html 13 MapReduce User Interface Â¨ Mapper Â¤ map n How many mapsï¼Ÿ Â¤ combine Â¨ Reducer Â¤ shuffle Â¤ sort/secondary sort Â¤ Reduce n How many reducesï¼Ÿ Â¨ Partitioner Â¨ Counter 14 MapReduce User Interface Â¨ Job Configuration Â¤ Job represents a MapReduce job configuration Â¨ Task Execution & Environment Â¤ The MRAppMaster executes the Mappper/Reducer task as a child process in a separate JVM. Â¨ Job Submission and Monitoring Â¤ Job.submit() or Job.waitForCompletion(boolean) Â¨ Job Input Â¨ Job Output 15 MapReduce User Interface Â¨ Other Useful Features Â¤ Submitting Jobs to Queues Â¤ Counters: global counters Â¤ DistributedCache: distribute application-specific, large, read-only files efficiently Â¤ Profiling Â¤ Debugging Â¤ Data Compression Â¤ Skipping Bad Records Â¤ â€¦â€¦ 16 MapReduceåŸºæœ¬å·¥ä½œè¿‡ç¨‹ 17 ä¸»è¦ç»„ä»¶ Â¨ æ–‡ä»¶è¾“å…¥æ ¼å¼InputFormat Â— å®šä¹‰äº†æ•°æ®æ–‡ä»¶å¦‚ä½•åˆ†å‰²å’Œè¯»å– Â— InputFormatæä¾›äº†ä»¥ä¸‹ä¸€äº›åŠŸèƒ½ Â— é€‰æ‹©æ–‡ä»¶æˆ–è€…å…¶å®ƒå¯¹è±¡ï¼Œç”¨æ¥ä½œä¸ºè¾“å…¥ Â— å®šä¹‰InputSplitsï¼Œå°†ä¸€ä¸ªæ–‡ä»¶åˆ†å¼€æˆä¸ºä»»åŠ¡ Â— ä¸ºRecordReaderæä¾›ä¸€ä¸ªå·¥å‚ï¼Œç”¨æ¥è¯»å–è¿™ä¸ªæ–‡ä»¶ Â— æœ‰ä¸€ä¸ªæŠ½è±¡çš„ç±»FileInputFormatï¼Œæ‰€æœ‰çš„è¾“å…¥æ ¼å¼ç±»éƒ½ä»è¿™ä¸ªç±»ç»§æ‰¿è¿™ä¸ªç±»çš„åŠŸèƒ½ä»¥åŠç‰¹æ€§ã€‚ å½“å¯åŠ¨ä¸€ä¸ªHadoopä»»åŠ¡çš„æ—¶å€™ï¼Œä¸€ä¸ªè¾“å…¥æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•è¢«è¾“å…¥åˆ°FileInputFormatå¯¹è±¡ä¸­ã€‚ FileInputFormatä»è¿™ä¸ªç›®å½•ä¸­è¯»å–æ‰€æœ‰æ–‡ä»¶ã€‚ç„¶åFileInputFormatå°†è¿™äº›æ–‡ä»¶åˆ†å‰²ä¸ºä¸€ä¸ªæˆ–è€…å¤š ä¸ªInputSplitsã€‚ Â— é€šè¿‡åœ¨JobConfå¯¹è±¡ä¸Šè®¾ç½®JobConf.setInputFormatè®¾ç½®æ–‡ä»¶è¾“å…¥çš„æ ¼å¼ 18 ä¸»è¦ç»„ä»¶ Â¨ æ–‡ä»¶è¾“å…¥æ ¼å¼InputFormat 19 InputFormat: Description: Key: Value: TextInputFormat Default format; reads lines of text files The byte offset of the line The line contents KeyValueTextInput Format Parses lines into key-val pairs Everything up to the first tab character The remainder of the line SequenceFileInputFormat A Hadoop-specific high- performance binary format user-defined user-defined ä¸»è¦ç»„ä»¶ Â¨ è¾“å…¥æ•°æ®åˆ†å—InputSplits Â— InputSplitå®šä¹‰äº†è¾“å…¥åˆ°å•ä¸ªMapä»»åŠ¡çš„è¾“å…¥æ•°æ® Â— ä¸€ä¸ªMapReduceç¨‹åºè¢«ç»Ÿç§°ä¸ºä¸€ä¸ªJobï¼Œå¯èƒ½æœ‰ ä¸Šç™¾ä¸ªä»»åŠ¡æ„æˆ Â— InputSplitå°†æ–‡ä»¶åˆ†ä¸º64MBçš„å¤§å° Â— é…ç½®æ–‡ä»¶hadoop-site.xmlä¸­çš„ mapred.min.split.sizeå‚æ•°æ§åˆ¶è¿™ä¸ªå¤§å° Â— mapred.tasktracker.map.task.maximumç”¨æ¥æ§åˆ¶æŸ ä¸€ä¸ªèŠ‚ç‚¹ä¸Šæ‰€æœ‰mapä»»åŠ¡çš„æœ€å¤§æ•°ç›® 20 ä¸»è¦ç»„ä»¶ 21 å…³äºSplitï¼ˆåˆ†ç‰‡ï¼‰ HDFS ä»¥å›ºå®šå¤§å°çš„block ä¸ºåŸºæœ¬å•ä½å­˜å‚¨æ•°æ®ï¼Œè€Œå¯¹äºMapReduce è€Œè¨€ï¼Œå…¶å¤„ç†å•ä½æ˜¯splitã€‚splitæ˜¯ä¸€ä¸ª é€»è¾‘æ¦‚å¿µï¼Œå®ƒåªåŒ…å«ä¸€äº›å…ƒæ•°æ®ä¿¡æ¯ï¼Œæ¯”å¦‚æ•°æ®èµ·å§‹ä½ç½®ã€æ•°æ®é•¿åº¦ã€æ•°æ®æ‰€åœ¨èŠ‚ç‚¹ç­‰ã€‚å®ƒçš„åˆ’åˆ†æ–¹æ³• å®Œå…¨ç”±ç”¨æˆ·è‡ªå·±å†³å®šã€‚ ä¸»è¦ç»„ä»¶ 22 Reduceä»»åŠ¡çš„æ•°é‡ Ã˜ æœ€ä¼˜çš„Reduceä»»åŠ¡ä¸ªæ•°å–å†³äºé›†ç¾¤ä¸­å¯ç”¨çš„reduceä»»åŠ¡æ§½(slot)çš„æ•°ç›® Ã˜ é€šå¸¸è®¾ç½®æ¯”reduceä»»åŠ¡æ§½æ•°ç›®ç¨å¾®å°ä¸€äº›çš„Reduceä»»åŠ¡ä¸ªæ•°ï¼ˆè¿™æ ·å¯ä»¥é¢„ç•™ä¸€äº›ç³»ç»Ÿèµ„æº å¤„ç†å¯èƒ½å‘ç”Ÿçš„é”™è¯¯ï¼‰ Mapä»»åŠ¡çš„æ•°é‡ Ã˜ Hadoopä¸ºæ¯ä¸ªsplitåˆ›å»ºä¸€ä¸ªMapä»»åŠ¡ï¼Œsplit çš„å¤šå°‘å†³å®šäº†Mapä»»åŠ¡çš„æ•°ç›®ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ ç†æƒ³çš„åˆ†ç‰‡å¤§å°æ˜¯ä¸€ä¸ªHDFSå— ä¸»è¦ç»„ä»¶ Â¨ æ•°æ®è®°å½•è¯»å…¥RecordReader Â— InputSplitå®šä¹‰äº†ä¸€é¡¹å·¥ä½œçš„å¤§å°ï¼Œä½†æ˜¯æ²¡æœ‰å®š ä¹‰å¦‚ä½•è¯»å–æ•°æ® Â— RecordReaderå®é™…ä¸Šå®šä¹‰äº†å¦‚ä½•ä»æ•°æ®ä¸Šè½¬åŒ– ä¸ºä¸€ä¸ª(key,value)å¯¹çš„è¯¦ç»†æ–¹æ³•ï¼Œå¹¶å°†æ•°æ®è¾“ å‡ºåˆ°Mapperç±»ä¸­ Â— TextInputFormatæä¾›äº†LineRecordReader 23 ä¸»è¦ç»„ä»¶ Â¨ Mapper Â— æ¯ä¸€ä¸ªMapperç±»çš„å®ä¾‹ç”Ÿæˆäº†ä¸€ä¸ªJavaè¿› ç¨‹ï¼ˆåœ¨æŸä¸€ä¸ªInputSplitä¸Šæ‰§è¡Œï¼‰ Â— æœ‰ä¸¤ä¸ªé¢å¤–çš„å‚æ•°OutputCollectorä»¥åŠ Reporterï¼Œå‰è€…ç”¨æ¥æ”¶é›†ä¸­é—´ç»“æœï¼Œåè€…ç”¨ æ¥è·å¾—ç¯å¢ƒå‚æ•°ä»¥åŠè®¾ç½®å½“å‰æ‰§è¡Œçš„çŠ¶æ€ã€‚ 24 ä¸»è¦ç»„ä»¶ Â¨ Combiner Â¤ åˆå¹¶ç›¸åŒkeyçš„é”®å€¼å¯¹ï¼Œå‡å°‘partitioneræ—¶å€™çš„æ•°æ®é€šä¿¡å¼€é”€ï¼› Â¤ æ˜¯åœ¨æœ¬åœ°æ‰§è¡Œçš„ä¸€ä¸ªReducerï¼Œæ»¡è¶³ä¸€å®šçš„æ¡ä»¶æ‰èƒ½å¤Ÿæ‰§è¡Œã€‚ Â— conf.setCombinerClass(Reduce.class); 25 ä¸»è¦ç»„ä»¶ Â¨ Partitioner & Shuffle Â¤ åœ¨Mapå·¥ä½œå®Œæˆä¹‹åï¼Œæ¯ä¸€ä¸ª Mapå‡½æ•°ä¼šå°†ç»“æœä¼ åˆ°å¯¹åº”çš„Reduceræ‰€åœ¨ çš„èŠ‚ç‚¹ï¼Œæ­¤æ—¶ï¼Œç”¨æˆ·å¯ä»¥æä¾›ä¸€ä¸ªPartitionerç±»ï¼Œç”¨æ¥å†³å®šä¸€ä¸ªç»™å®šçš„ (key,value)å¯¹ä¼ è¾“çš„å…·ä½“ä½ç½®ã€‚ 26 ä¸»è¦ç»„ä»¶ Â¨ Sort Â¤ ä¼ è¾“åˆ°æ¯ä¸€ä¸ªèŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰çš„Reduceå‡½æ•°æ¥æ”¶åˆ°çš„(key,value)éƒ½ä¼šè¢«Hadoop è‡ªåŠ¨æ’åºï¼ˆå³Mapç”Ÿæˆçš„ç»“æœä¼ é€åˆ°æŸä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶å€™ï¼Œä¼šè¢«è‡ªåŠ¨æ’åºï¼‰ 27 ä¸»è¦ç»„ä»¶ 28 è¾“å…¥ ç¼“å­˜ æº¢å†™ï¼ˆåˆ†åŒºã€æ’åºã€åˆå¹¶ï¼‰ æ•°æ®è¢«Reduce ä»»åŠ¡å–èµ° ç£ç›˜æ–‡ä»¶å½’å¹¶ å¤šä¸ªåˆ†åŒº å½’å¹¶ å½’å¹¶ è¾“å‡º Reduceä»»åŠ¡Mapä»»åŠ¡ å…¶ä»–Mapä»»åŠ¡ å…¶ä»–Reduceä»»åŠ¡ Map Reduce 1. Shuffleè¿‡ç¨‹ç®€ä»‹ ä¸»è¦ç»„ä»¶ 29 2. Mapç«¯çš„Shuffleè¿‡ç¨‹ Mapä»»åŠ¡ ç¼“å­˜ 1 2 è¾“å…¥æ•°æ®å’Œæ‰§è¡ŒMapä»»åŠ¡ å†™å…¥ç¼“å­˜ 3 æº¢å†™ï¼ˆåˆ†åŒºã€æ’åºã€åˆå¹¶ï¼‰ 4 æ–‡ä»¶å½’å¹¶ Ã˜ æ¯ä¸ªMapä»»åŠ¡åˆ†é…ä¸€ä¸ªç¼“å­˜ Ã˜ MapReduceé»˜è®¤100MBç¼“å­˜ Ã˜ è®¾ç½®æº¢å†™æ¯”ä¾‹0.8 Ã˜ åˆ†åŒºé»˜è®¤é‡‡ç”¨å“ˆå¸Œå‡½æ•° Ã˜ æ’åºæ˜¯é»˜è®¤çš„æ“ä½œ Ã˜ æ’åºåå¯ä»¥åˆå¹¶ï¼ˆCombineï¼‰ Ã˜ åˆå¹¶ä¸èƒ½æ”¹å˜æœ€ç»ˆç»“æœ Ã˜ åœ¨Mapä»»åŠ¡å…¨éƒ¨ç»“æŸä¹‹å‰è¿›è¡Œå½’å¹¶ Ã˜ å½’å¹¶å¾—åˆ°ä¸€ä¸ªå¤§çš„æ–‡ä»¶ï¼Œæ”¾åœ¨æœ¬åœ°ç£ç›˜ Ã˜ æ–‡ä»¶å½’å¹¶æ—¶ï¼Œå¦‚æœæº¢å†™æ–‡ä»¶æ•°é‡å¤§äºé¢„å®šå€¼ï¼ˆé»˜è®¤æ˜¯ 3ï¼‰åˆ™å¯ä»¥å†æ¬¡å¯åŠ¨Combinerï¼Œå°‘äº3ä¸éœ€è¦ Ã˜ JobTrackerä¼šä¸€ç›´ç›‘æµ‹Mapä»»åŠ¡çš„æ‰§è¡Œï¼Œå¹¶é€šçŸ¥ Reduceä»»åŠ¡æ¥é¢†å–æ•°æ® åˆå¹¶ï¼ˆCombineï¼‰å’Œå½’å¹¶ï¼ˆMergeï¼‰çš„åŒºåˆ«ï¼š ä¸¤ä¸ªé”®å€¼å¯¹<â€œaâ€,1>å’Œ<â€œaâ€,1>ï¼Œå¦‚æœåˆå¹¶ï¼Œä¼šå¾—åˆ°<â€œaâ€,2>ï¼Œå¦‚æœå½’å¹¶ï¼Œä¼šå¾—åˆ°<â€œaâ€,<1,1>> ä¸»è¦ç»„ä»¶ 30 3. Reduceç«¯çš„Shuffleè¿‡ç¨‹ ç¼“å­˜ Mapä»»åŠ¡ Reduceä»»åŠ¡ ç£ç›˜ ç£ç›˜ æ–‡ä»¶å½’å¹¶ å…¶ä»–Mapä»»åŠ¡ â€œ é¢†å–â€ æ•°æ® å½’å¹¶æ•°æ® æŠŠæ•°æ®è¾“å…¥ç»™ Reduceä»»åŠ¡ 1 2 3 å…¶ä»–Reduceä»»åŠ¡ åˆ†åŒº åˆ†åŒº å…¶ä»–Reduceä»»åŠ¡ Ã˜ Reduceä»»åŠ¡å‘JobTrackerè¯¢é—®Mapä»»åŠ¡æ˜¯å¦å·²ç»å®Œæˆï¼Œè‹¥å®Œæˆï¼Œåˆ™é¢†å–æ•°æ® Ã˜ Reduceé¢†å–æ•°æ®å…ˆæ”¾å…¥ç¼“å­˜ï¼Œæ¥è‡ªä¸åŒMapæœºå™¨ï¼Œå…ˆå½’å¹¶ï¼Œå†åˆå¹¶ï¼Œå†™å…¥ç£ç›˜ Ã˜ å¤šä¸ªæº¢å†™æ–‡ä»¶å½’å¹¶æˆä¸€ä¸ªæˆ–å¤šä¸ªå¤§æ–‡ä»¶ï¼Œæ–‡ä»¶ä¸­çš„é”®å€¼å¯¹æ˜¯æ’åºçš„ Ã˜ å½“æ•°æ®å¾ˆå°‘æ—¶ï¼Œä¸éœ€è¦æº¢å†™åˆ°ç£ç›˜ï¼Œç›´æ¥åœ¨ç¼“å­˜ä¸­å½’å¹¶ï¼Œç„¶åè¾“å‡ºç»™Reduce ä¸»è¦ç»„ä»¶ Â¨ Reducer Â— æ‰§è¡Œç”¨æˆ·å®šä¹‰çš„Reduceæ“ä½œ Â— æ¥æ”¶åˆ°ä¸€ä¸ªOutputCollectorçš„ç±»ä½œä¸ºè¾“å‡º 31 ä¸»è¦ç»„ä»¶ Â¨ æ–‡ä»¶è¾“å‡ºæ ¼å¼OutputFormat Â— å†™å…¥åˆ°HDFSçš„æ‰€æœ‰OutputFormatéƒ½ç»§æ‰¿è‡ª FileOutputFormat Â— æ¯ä¸€ä¸ªReduceréƒ½å†™ä¸€ä¸ªæ–‡ä»¶åˆ°ä¸€ä¸ªå…±åŒçš„è¾“ å‡ºç›®å½•ï¼Œæ–‡ä»¶åæ˜¯part-nnnnnï¼Œå…¶ä¸­nnnnnæ˜¯ ä¸æ¯ä¸€ä¸ªreducerç›¸å…³çš„ä¸€ä¸ªå·ï¼ˆpartition idï¼‰ Â— FileOutputFormat.setOutputPath() Â— JobConf.setOutputFormat() 32 ä¸»è¦ç»„ä»¶ Â¨ æ–‡ä»¶è¾“å‡ºæ ¼å¼OutputFormat Â¨ RecordWriter Â¤ TextOutputFormatå®ç°äº†ç¼ºçœçš„LineRecordWriterï¼Œä»¥â€key\\t valueâ€å½¢å¼è¾“å‡ºä¸€ è¡Œç»“æœ 33 OutputFormat: Description TextOutputFormat Default; writes lines in \"key \\t value\" form SequenceFileOutputFormat Writes binary files suitable for reading into subsequent MapReduce jobs NullOutputFormat Disregards its inputs MapReduce WordCount1.0 Â¨ åŸºæœ¬æ•°æ®å¤„ç†æµç¨‹ 34 MapReduce WordCount1.0 Â¨ ç¨‹åºå‘˜ä¸»è¦çš„ç¼–ç å·¥ä½œå¦‚ä¸‹ï¼š Â¤ å®ç°Mapç±» Â¤ å®ç°Reduceç±» Â¤ å®ç°mainå‡½æ•°ï¼ˆè¿è¡ŒJobï¼‰ 35 MapReduce WordCount1.0 Â¤ å®ç°Mapç±» Â¤ è¿™ä¸ªç±»å®ç° org.apache.hadoop.mapreduce.Mapperä¸­çš„ map æ–¹æ³•ï¼Œè¾“å…¥å‚æ•°ä¸­çš„ value æ˜¯æ–‡æœ¬æ–‡ä»¶ä¸­çš„ä¸€è¡Œï¼Œåˆ©ç”¨ StringTokenizer å°†è¿™ä¸ªå­—ç¬¦ä¸²æ‹†æˆå•è¯ï¼Œç„¶åé€šè¿‡context.w riteæ”¶é›†<key, value>å¯¹ã€‚ Â¤ ä»£ç ä¸­ LongWritable, IntWritable, Text å‡æ˜¯ Hadoop ä¸­å®ç°çš„ç”¨äºå°è£… Java æ•°æ®ç±»å‹çš„ ç±»ï¼Œè¿™äº›ç±»éƒ½èƒ½å¤Ÿè¢«ä¸²è¡ŒåŒ–ä»è€Œä¾¿äºåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿›è¡Œæ•°æ®äº¤æ¢ï¼Œå¯ä»¥å°†å®ƒä»¬åˆ†åˆ« è§†ä¸º long, int, String çš„æ›¿ä»£ã€‚ MapReduce WordCount1.0 Â¨ Mapç±»ä»£ç  public static class TokenizerMapper //å®šä¹‰Mapç±»å®ç°å­—ç¬¦ä¸²åˆ†è§£ extends Mapper<Object, Text, Text, IntWritable>{ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); //å®ç°map()å‡½æ•° public void map(Object key, Text value, Context context) throws IOException, InterruptedException { //å°†å­—ç¬¦ä¸²æ‹†è§£æˆå•è¯ StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); //å°†åˆ†è§£åçš„ä¸€ä¸ªå•è¯å†™å…¥word context.write(word, one); //æ”¶é›†<key, value> } } } MapReduce WordCount1.0 Â¨ å®ç°Reduceç±» Â¤ è¿™ä¸ªç±»å®ç°org.apache.hadoop.mapreduce.Reducerä¸­çš„ reduce æ–¹æ³•ï¼Œè¾“å…¥å‚æ•°ä¸­çš„(key, values) æ˜¯ç”± Map ä»»åŠ¡è¾“å‡ºçš„ä¸­é—´ç»“æœï¼Œvalues æ˜¯ä¸€ä¸ªIteratorï¼Œéå†è¿™ä¸ª Iteratorï¼Œå°±å¯ ä»¥å¾—åˆ°å±äºåŒä¸€ä¸ª keyçš„æ‰€æœ‰valueã€‚ Â¤ æ­¤å¤„key æ˜¯ä¸€ä¸ªå•è¯ï¼Œvalue æ˜¯è¯é¢‘ã€‚åªéœ€è¦å°†æ‰€æœ‰çš„ value ç›¸åŠ ï¼Œå°±å¯ä»¥å¾—åˆ°è¿™ä¸ªå• è¯çš„æ€»çš„å‡ºç°æ¬¡æ•°ã€‚ MapReduce WordCount1.0 Â¨ Reduceç±»ä»£ç  //Reducekeyvalue public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> { private IntWritable result = new IntWritable(); //reduce() public void reduce(Text key, Iterable<IntWritable> values, Context context ) throws IOException, InterruptedException { int sum = 0; //valueskeyvalue for (IntWritable val : values) { sum += val.get(); } result.set(sum); //<key, value> context.write(key, result); } } MapReduce WordCount1.0 Â¨ å®ç°mainå‡½æ•°ï¼ˆè¿è¡ŒJobï¼‰ Â¤ åœ¨ Hadoop ä¸­ä¸€æ¬¡è®¡ç®—ä»»åŠ¡ç§°ä¹‹ä¸ºä¸€ä¸ª Jobï¼Œmainå‡½æ•°ä¸»è¦è´Ÿè´£æ–°å»ºä¸€ä¸ªJobå¯¹è±¡å¹¶ä¸º ä¹‹è®¾å®šç›¸åº”çš„Mapperå’ŒReducerç±»ï¼Œä»¥åŠè¾“å…¥ã€è¾“å‡ºè·¯å¾„ç­‰ã€‚ MapReduce WordCount1.0 Â¨ mainå‡½æ•°ä»£ç  public static void main(String[] args) throws Exception{ //ä¸ºä»»åŠ¡è®¾å®šé…ç½®æ–‡ä»¶ Configuration conf = new Configuration(); //å‘½ä»¤è¡Œå‚æ•° String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length != 2){ System.err.println(\"Usage: wordcount <in> <out>\"); System.exit(2); } Job job = new Job(conf, â€œword countâ€); //æ–°å»ºä¸€ä¸ªç”¨æˆ·å®šä¹‰çš„Job job.setJarByClass(WordCount.class); //è®¾ç½®æ‰§è¡Œä»»åŠ¡çš„jar job.setMapperClass(TokenizerMapper.class); //è®¾ç½®Mapperç±» job.setCombinerClass(IntSumReducer.class); //è®¾ç½®Combineç±» job.setReducerClass(IntSumReducer.class); //è®¾ç½®Reducerç±» job.setOutputKeyClass(Text.class); //è®¾ç½®jobè¾“å‡ºçš„key //è®¾ç½®jobè¾“å‡ºçš„value job.setOutputValueClass(IntWritable.class); //è®¾ç½®è¾“å…¥æ–‡ä»¶çš„è·¯å¾„ FileInputFormat.addInputPath(job, new Path(otherArgs[0])); //è®¾ç½®è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„ FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); //æäº¤ä»»åŠ¡å¹¶ç­‰å¾…ä»»åŠ¡å®Œæˆ System.exit(job.waitForCompletion(true) ? 0 : 1); } MapReduce WordCount1.0 Â¨ ç¼–è¯‘æºä»£ç  Â¤ å®Œæˆç¼–è¯‘ Â¤ å¯¼å‡ºjaræ–‡ä»¶ n å¯¼å‡ºwordcountç¨‹åºçš„jaråŒ… n å¯¼å‡ºjaræ–‡ä»¶çš„æ—¶å€™å¯ä»¥æŒ‡å®šä¸€ä¸ª ä¸»ç±»MainClassï¼Œä½œä¸ºé»˜è®¤æ‰§è¡Œçš„ ä¸€ä¸ªç±» 42 IntelliJ IDEA CE MapReduce WordCount1.0 Â¨ æœ¬åœ°è¿è¡Œè°ƒè¯• Â¤ å°†ç¨‹åºå¤åˆ¶åˆ°æœ¬åœ°Hadoopç³»ç»Ÿçš„æ‰§è¡Œç›®å½•ï¼Œå¹¶å‡†å¤‡ä¸€ä¸ªå°çš„æµ‹è¯• æ•°æ®ï¼Œå³å¯é€šè¿‡hadoopçš„å®‰è£…åŒ…è¿›è¡Œè¿è¡Œè°ƒè¯• bin/hadoop fs -mkdir input bin/hadoop fs -put docs/*.html input bin/hadoop jar example.jar wordcount input output Â¤ å½“éœ€è¦ç”¨é›†ç¾¤è¿›è¡Œæµ·é‡æ•°æ®å¤„ç†æ—¶ï¼Œåœ¨æœ¬åœ°ç¨‹åºè°ƒè¯•æ­£ç¡® è¿è¡Œåï¼Œå¯æŒ‰ç…§å‰è¿°çš„è¿œç¨‹ä½œä¸šæäº¤æ­¥éª¤ï¼Œå°†ä½œä¸šæäº¤åˆ° è¿œç¨‹hadoopé›†ç¾¤ä¸Šè¿è¡Œã€‚ 43 å¼€å‘ç¯å¢ƒä¸å·¥å…·ï¼šVS Code 44 å®‰è£…Javaå¼€å‘æ’ä»¶å’ŒMavenæ’ä»¶ å¼€å‘ç¯å¢ƒä¸å·¥å…·ï¼šVS Code 45 ç¼–å†™ã€ç¼–è¯‘ã€æ‰“åŒ…ã€è¿è¡ŒJavaç¨‹åº MapReduce WordCount 2.0 Â¨ 1. å¿½ç•¥å¤§å°å†™ Â¨ 2. å¿½ç•¥æ ‡ç‚¹ç¬¦å· 46 MapReduce WordCount 2.0 Â¨ Mapper (new) 47 MapReduce WordCount 2.0 Â¨ Mapper (new) 48 MapReduce WordCount 2.0 Â¨ Mapper (updated) 49 MapReduce WordCount 2.0 Â¨ Reducer 50 MapReduce WordCount 2.0 Â¨ mainå‡½æ•° 51 MapReduce WordCount 2.0 Â¨ Input Â¤ File01: Hello World Bye World Â¤ File02: Hello Hadoop Goodbye Hadoop Â¤ File03: Hello World, Bye World! Â¤ File04: Hello Hadoop, Goodbye to hadoop. Â¨ è¿è¡Œ $bin/hadoop jar wc2.jar input output2 Â¨ ç»“æœï¼š 52 MapReduce WordCount 2.0 Â¨ å‡†å¤‡patternsæ–‡ä»¶ Â¨ å†æ¬¡è¿è¡Œ Â¤ $ bin/hadoop jar wc.jar - Dwordcount.case.sensitive=true input output3 -skip wordcount/patterns.txt Â¨ è¾“å‡ºï¼š 53 Â¨ å†æ¬¡è¿è¡Œ Â¤ $ bin/hadoop jar wc.jar - Dwordcount.case.sensitive=false input output4 -skip wordcount/patterns.txt Â¨ è¾“å‡ºï¼š MapReduce WordCount 2.0 Â¨ Demonstrates how applications can access configuration parameters in the setup method of the Mapper (and Reducer) implementations. Â¨ Demonstrates how the DistributedCache can be used to distribute read-only data needed by the jobs. Here it allows the user to specify word-patterns to skip while counting. Â¨ Demonstrates the utility of the GenericOptionsParser to handle generic Hadoop command-line options. Â¨ Demonstrates how applications can use Counters and how they can set application- specific status information passed to the map (and reduce) method. 54 MapReduce WordCount in Python Â¨ Hadoop Streaming Â¤ Hadoop Streamingæ˜¯Hadoopçš„ä¸€ä¸ªå·¥å…·ï¼Œ å®ƒå¸®åŠ©ç”¨æˆ·åˆ›å»ºå’Œè¿è¡Œä¸€ç±» ç‰¹æ®Šçš„map/reduceä½œä¸šï¼Œ è¿™äº›ç‰¹æ®Šçš„map/reduceä½œä¸šæ˜¯ç”±ä¸€äº›å¯æ‰§è¡Œ æ–‡ä»¶æˆ–è„šæœ¬æ–‡ä»¶å……å½“mapperæˆ–è€…reducerã€‚ Â¤ Hadoop Streaming å‚è€ƒæ–‡æ¡£ n https://hadoop.apache.org/docs/current/hadoop- streaming/HadoopStreaming.html 55 Hadoop Streaming Â¨ ä½¿ç”¨æ–¹å¼ Â¤ $HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/.../hadoop-streaming.jar [genericOptions] [streamingOptions] Â¤ $HADOOP_HOME/bin/ mapred streaming [genericOptions] [streamingOptions] Â¨ å¸¸ç”¨å‚æ•° Â¤ -file Â¤ -mapper Â¤ -reducer Â¤ -input Â¤ -output 56 Hadoop Streaming Â¨ ä½¿ç”¨ Hadoop Streaming æ¥è®©æ•°æ®åœ¨map å’Œ reduceä¹‹é—´ä¼ é€’æ•°æ®ï¼Œä½¿ç”¨sys.stdin è·å¾—è¾“å…¥æ•°æ®ï¼Œsys.stdout ä½œä¸ºæ ‡å‡†è¾“å‡ºã€‚ Â¨ mapperå’Œreduceréƒ½æ˜¯å¯æ‰§è¡Œæ–‡ä»¶ï¼Œå®ƒä»¬ä»æ ‡å‡†è¾“å…¥è¯»å…¥æ•°æ®ï¼ˆä¸€è¡Œä¸€è¡Œè¯»ï¼‰ï¼Œ å¹¶æŠŠè®¡ç®—ç»“æœå‘ç»™æ ‡å‡†è¾“å‡ºã€‚Streamingå·¥å…·ä¼šåˆ›å»ºä¸€ä¸ªMap/Reduceä½œä¸šï¼Œ å¹¶ æŠŠå®ƒå‘é€ç»™åˆé€‚çš„é›†ç¾¤ï¼ŒåŒæ—¶ç›‘è§†è¿™ä¸ªä½œä¸šçš„æ•´ä¸ªæ‰§è¡Œè¿‡ç¨‹ã€‚ Â¨ ç”¨æˆ·å¯ä»¥è®¾å®šstream.non.zero.exit.is.failureä¸ºtrue æˆ–false æ¥è¡¨æ˜streaming taskçš„ è¿”å›å€¼éé›¶æ—¶æ˜¯ Failure è¿˜æ˜¯Successã€‚é»˜è®¤æƒ…å†µï¼Œstreaming taskè¿”å›éé›¶æ—¶è¡¨ç¤º å¤±è´¥ã€‚ 57 Hadoop Streaming 58 mapper.py 59 reducer.py 60 è¿è¡Œ #!/bin/bash bin/hdfs dfs -rm -r python/output bin/hadoop jar ./share/hadoop/tools/lib/hadoop-streaming-2.7.4.jar \\ -D stream.non.zero.exit.is.failure=false \\ -D mapred.job.name=python_word_count \\ -file ./mapper.py -mapper \"python mapper.py\" \\ -file ./reducer.py -reducer \"python reducer.py\" \\ -input python/input/wordcount_data.txt -output python/output 61 MapReduceçŸ©é˜µä¹˜æ³• Â¨ å¹¶è¡ŒåŒ–çŸ©é˜µä¹˜æ³• Â¨ çŸ©é˜µMabå’ŒçŸ©é˜µNbcçš„ä¹˜ç§¯P=MÂ·N Â¤ ğ‘ƒ!\" = (ğ‘€ % ğ‘)!\"= âˆ‘ğ‘š!#ğ‘›#\" Â¨ Mapé˜¶æ®µï¼šè¿›è¡Œæ•°æ®å‡†å¤‡ Â¤ å®šä¹‰(key, value)å¯¹ Â¤ çŸ©é˜µMï¼š<(i, k), (M, j, mij)> Â¤ çŸ©é˜µNï¼š<(i, k), (N, j, njk)> Â¤ å…¶ä¸­i=1, 2, â€¦a; j=1,2,â€¦b; k=1, 2, â€¦c 62 MapReduceçŸ©é˜µä¹˜æ³• Â¨ Reduceé˜¶æ®µï¼šmij*njk Â¤ å¯¹äºæ¯ä¸ªé”®(i, k)ç›¸å…³è”çš„å€¼(M, j, mij)åŠ(N, j, njk)ï¼Œæ ¹æ®ç›¸åŒjå€¼å°†mijå’Œ njkåˆ†åˆ«å­˜å…¥ä¸åŒæ•°ç»„ä¸­ï¼Œç„¶åå°†ä¸¤è€…çš„ç¬¬jä¸ªå…ƒç´ æŠ½å–å‡ºæ¥åˆ†åˆ« ç›¸ä¹˜ï¼Œæœ€åç›¸åŠ ï¼Œå³å¯å¾—åˆ°pikçš„å€¼ã€‚ Â¨ ä»£ç ç¤ºä¾‹ 63 MapReduceçŸ©é˜µä¹˜æ³• Â¨ ä¸¾ä¾‹ï¼š Â¨ M 1,2 N 2 1 3 0 2 4 Â¨ i=1 j=1,2 k=1,2,3 Â¨ Mapè¾“å‡º n <(1,1),(M,1,1)><(1,1),(N,1,2)><(1,1),(M,2,2)><(1,1),(N,2,0)> n <(1,2),(M,1,1)><(1,2),(N,1,1)><(1,2),(M,2,2)><(1,2),(N,2,2)> n <(1,3),(M,1,1)><(1,3),(N,1,3)><(1,3),(M,2,2)><(1,3),(N,2,4)> Â¨ Reduceè¾“å‡º n <(1,1),2> <(1,2),5><(1,3),11> 64 å…³ç³»ä»£æ•°è¿ç®— Â¨ é€‰æ‹©ã€æŠ•å½±ã€å¹¶ã€äº¤ã€å·®ä»¥åŠè‡ªç„¶è¿æ¥æ“ä½œ 65 ID NAME AGE GRADE 1 å¼ å°é›… 20 91 2 åˆ˜ä¼Ÿ 19 87 3 æå©· 21 82 4 å­™å¼º 20 95 è¡¨1 å…³ç³»R ID GENDER HEIGHT 1 å¥³ 165 2 ç”· 178 3 å¥³ 170 4 ç”· 175 è¡¨2 å…³ç³»S å…³ç³»ä»£æ•°è¿ç®— Â¨ é€‰æ‹©æ“ä½œ Â¤ Mapé˜¶æ®µï¼šå¯¹äºæ¯ä¸ªè¾“å…¥çš„è®°å½•åˆ¤æ–­æ˜¯å¦æ»¡è¶³æ¡ä»¶ï¼Œå°†æ»¡è¶³æ¡ä»¶çš„è®° å½•è¾“å‡ºä¸º(Rc, null)ã€‚ Â¤ Reduceé˜¶æ®µï¼šæ— éœ€åšé¢å¤–å·¥ä½œ Â¤ setNumReduceTasks(0) Â¨ æŠ•å½±æ“ä½œ Â¤ Map é˜¶æ®µï¼šå°†æ¯æ¡è®°å½•åœ¨è¯¥å±æ€§ä¸Šçš„å€¼ä½œä¸ºé”®è¾“å‡ºå³å¯ã€‚ Â¤ Reduceé˜¶æ®µï¼šå°†Mapç«¯è¾“å…¥çš„é”®è¾“å‡ºå³å¯ã€‚ 66 å…³ç³»ä»£æ•°è¿ç®— Â¨ äº¤è¿ç®— Â¤ åŒä¸€ä¸ªæ¨¡å¼çš„å…³ç³»Rå’Œå…³ç³»Tæ±‚äº¤é›† Â¤ Mapé˜¶æ®µï¼šæ¯æ¡è®°å½•rè¾“å‡ºä¸º(r, 1) Â¤ Reduceé˜¶æ®µï¼šå¦‚æœè®¡æ•°ä¸º2åˆ™è¾“å‡ºè¯¥è®°å½• Â¤ æ³¨æ„äº‹é¡¹ï¼šRå’ŒTçš„ç›¸åŒè®°å½•è¦å‘é€åˆ°åŒä¸€ä¸ªReduceèŠ‚ç‚¹ n é‡å†™hashCode()æ–¹æ³•ä½¿å¾—å…·æœ‰ç›¸åŒåŸŸå€¼çš„è®°å½•å…·æœ‰ç›¸åŒçš„å“ˆå¸Œå€¼ 67 å…³ç³»ä»£æ•°è¿ç®— Â¨ å·®è¿ç®— Â¤ åŒä¸€ä¸ªæ¨¡å¼çš„å…³ç³»Rå’Œå…³ç³»Tæ±‚å·®ï¼ˆR-Tï¼‰ Â¤ Mapé˜¶æ®µï¼šRä¸­çš„è®°å½•rè¾“å‡ºé”®å€¼å¯¹(r, R)ï¼ŒTä¸­çš„è®°å½•rè¾“å‡ºé”®å€¼å¯¹(r, T) Â¤ Reduceé˜¶æ®µï¼šå¦‚æœåªæœ‰Rè€Œæ²¡æœ‰Tï¼Œåˆ™å°†è¯¥è®°å½•è¾“å‡º Â¤ æ³¨æ„äº‹é¡¹ï¼šRå’ŒTçš„ç›¸åŒè®°å½•è¦å‘é€åˆ°åŒä¸€ä¸ªReduceèŠ‚ç‚¹ n é‡å†™hashCode()æ–¹æ³•ä½¿å¾—å…·æœ‰ç›¸åŒåŸŸå€¼çš„è®°å½•å…·æœ‰ç›¸åŒçš„å“ˆå¸Œå€¼ 68 å…³ç³»ä»£æ•°è¿ç®— Â¨ è‡ªç„¶è¿æ¥ Â¤ æ¯”å¦‚åœ¨å±æ€§IDä¸Šåšå…³ç³»Rå’Œå…³ç³»Sçš„è‡ªç„¶è¿æ¥ Â¤ Mapé˜¶æ®µï¼šå°†IDçš„å€¼ä½œä¸ºkeyï¼Œå°†å…¶ä½™å±æ€§çš„å€¼ä»¥åŠRçš„åç§°ï¼ˆSä¸­çš„è®° å½•ä¸ºSçš„åç§°ï¼‰ä½œä¸ºvalue Â¤ Reduceé˜¶æ®µï¼šå°†åŒä¸€keyä¸­æ‰€æœ‰çš„å€¼æ ¹æ®å®ƒä»¬çš„æ¥æºï¼ˆRæˆ–Sï¼‰åˆ†ä¸ºä¸¤ç»„ åšç¬›å¡å°”ä¹˜ç§¯ç„¶åè¾“å‡ºã€‚ Â¨ ä»£ç ç¤ºä¾‹ 69","libVersion":"0.3.2","langs":""}