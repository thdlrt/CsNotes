> [动手学深度学习](https://courses.d2l.ai/zh-v2/)

- 线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常**包含多个最小值**。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。
### 多层感知机
- 单层的神经网络只能处理线性关系（线性回归模型），通过添加隐藏层可以处理更加普遍的函数关系类型
	- 这种架构就成为多层感知机 MLP
	- ![image.png|400](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250306175311.png)
- 为了让多层结构真正提升模型的表示能力（而不是只能表示线性的仿射关系），对每个隐藏单元应用非线性的激活函数（如ReLU、Sigmoid），从而阻止多层感知机退化成线性模型 $\mathbf{H}=\sigma(\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)})$
	- ReLU: 修正线性单元 $\mathrm{ReLU}(x)=\max(x,0)$。它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。
	- sigmoid：机爱你个 R 上的输入映射到 $(0,1)$，$\mathrm{sigmoid}(x)=\frac1{1+\exp(-x)}.$在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。
	- tanh：将输入压缩到 $(-1,1)$，$\tanh(x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}$
- 过使用更深的网络，可以**更容易地逼近许多函数**。
#### 实现
```python
# 创建多层网络
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);

batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

#### 暂退法
- 希望模型深度挖掘特征，将权重**分散**到许多特征中，而不是过于依赖少数潜在的虚假关联
- 线性模型没有考虑到特征之间的交互作用，对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。
- 泛化性和灵活性之间的这种基本权衡被描述为**偏差-方差权衡**
- 线性模型
	- 很高的偏差：它们只能表示一小类函数
	- 方差很低：它们在不同的随机数据样本上可以得出相似的结果。
- 深度神经网络：不局限于单独查看每个特征，而是学习**特征之间的交互**

- 在标准暂退法正则化中，通过按**保留**（未丢弃）的节点的分数进行规范化来消除每一层的偏差
	- 每个中间活性值 $h$ 以暂退概率 $p$ 由随机变量 $h`$ 替代
	- $h^{\prime}=\begin{cases}0&\text{概率为 }p\\\frac h{1-p}&\text{其他情况}&\end{cases}$
```python
def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)
```
- 只需在每个全连接层之后添加一个 `Dropout` 层，将暂退概率作为唯一的参数传递给它的构造函数。在训练时，`Dropout` 层将根据指定的暂退概率随机丢弃上一层的输出。在测试时，`Dropout` 层仅传递数据(测试时通常不进行丢弃)。
```python
net = nn.Sequential(nn.Flatten(),
        nn.Linear(784, 256),
        nn.ReLU(),
        # 在第一个全连接层之后添加一个dropout层
        nn.Dropout(dropout1),
        nn.Linear(256, 256),
        nn.ReLU(),
        # 在第二个全连接层之后添加一个dropout层
        nn.Dropout(dropout2),
        nn.Linear(256, 10))
```
#### 计算图与传播
##### 前向传播
- 从输入层到输出层按顺序来计算和存储神经网络中每层的结果
##### 反向传播
- 计算神经网络采纳数梯度的方法，根据微积分中的链式规则，按早相反的顺序从输出层到输入层遍历网络，存储了计算某些参数梯度时所需的任何中间变量
#### 数值稳定性和模型初始化
- 神经网络的结构如下：每一层都是上一层输出和本层参数为变量的函数 $\mathbb{H}^{(l)}=f(\mathbb{H}^{(l-1)};\mathbb{W}^{(l)})$
	- 那么网络的输出就可以表示为 $\mathbb{H}^{(L)}=f(f(\cdots f(\mathbb{X};\mathbb{W}^{(1)})\cdots;\mathbb{W}^{(L-1)});\mathbb{W}^{(L)})$
	- 对 $l$ 参数的更新就依赖梯度 $\frac{\partial\mathcal{L}}{\partial\mathbb{W}^{(l)}}=\frac{\partial\mathcal{L}}{\partial\mathbb{H}^{(L)}}\cdot\prod_{i=l}^{L-1}\frac{\partial\mathbb{H}^{(i+1)}}{\partial\mathbb{H}^{(i)}}$
- 梯度是一个**累积乘积**，每一层的梯度都依赖于前面所有层的梯度，通过反向传播算法计算梯度时，梯度会随着层数增加而出现**指数性衰减或增长**
	- 如果每一层的梯度的值非常小或非常大，那么它们的乘积会导致梯度整体上指数性地减小或增大。
- 梯度消失
	  - 当每一层的梯度都小于 1，就可能导致梯度消失
	- 当sigmoid函数的输入很大或是很小时，它的梯度都会消失，因此更稳定的ReLU系列函数已经成为从业者的默认选择
	- **靠近输入层的参数更新非常缓慢**：因为梯度接近 0，导致这些层的权重几乎不会变化，模型无法有效学习深层特征。
	  - **训练时间变长或停止**：模型可能无法收敛。
- 梯度爆炸
	- 如果权重初始化得过大，或者激活函数（如 ReLU）在大范围值上表现不稳定，那么每一层的梯度值会变大。
	- 如果网络的权重在初始化时值很大，正向传播中的激活值会不断增大，反向传播的梯度也会随之增大，导致梯度爆炸。
	- **参数更新过大**：导致模型的损失函数值变得不稳定，甚至出现 NaN。
	- **优化难以收敛**：爆炸的梯度会破坏优化算法的稳定性。
##### 参数初始化
- 假设我们有一个简单的多层感知机，它有一个隐藏层和两个隐藏单元，如果我们在隐藏层的所有单元的参数初始化时，给定相同的值，这就会导致一种对称性
	- 由于隐藏单元的输出完全相同，网络的行为就好像隐藏层只有一个单元，失去了隐藏单元的多样性。
	- 可以通过**随机初始化和暂退法正则化**来解决
- 更好的初始化方案：Xavier 初始化
	- 通过控制权重的方差，确保前向传播和反向传播中方差的稳定性，从而减轻梯度消失或梯度爆炸问题。
#### 环境和分布偏移

### 深度学习计算
### 卷积神经网络