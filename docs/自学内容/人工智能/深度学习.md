> [动手学深度学习](https://courses.d2l.ai/zh-v2/)

- 线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常**包含多个最小值**。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。
### 多层感知机
- 单层的神经网络只能处理线性关系（线性回归模型），通过添加隐藏层可以处理更加普遍的函数关系类型
	- 这种架构就成为多层感知机 MLP
	- ![image.png|400](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250306175311.png)
- 为了让多层结构真正提升模型的表示能力（而不是只能表示线性的仿射关系），对每个隐藏单元应用非线性的激活函数（如ReLU、Sigmoid），从而阻止多层感知机退化成线性模型 $\mathbf{H}=\sigma(\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)})$
	- ReLU: 修正线性单元 $\mathrm{ReLU}(x)=\max(x,0)$。它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。
	- sigmoid：机爱你个 R 上的输入映射到 $(0,1)$，$\mathrm{sigmoid}(x)=\frac1{1+\exp(-x)}.$在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。
	- tanh：将输入压缩到 $(-1,1)$，$\tanh(x)=\frac{1-\exp(-2x)}{1+\exp(-2x)}$
- 过使用更深的网络，可以**更容易地逼近许多函数**。
#### 实现
```python
# 创建多层网络
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);

batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

#### 暂退法
- 希望模型深度挖掘特征，将权重**分散**到许多特征中，而不是过于依赖少数潜在的虚假关联
- 线性模型没有考虑到特征之间的交互作用，对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。
- 泛化性和灵活性之间的这种基本权衡被描述为**偏差-方差权衡**
- 线性模型
	- 很高的偏差：它们只能表示一小类函数
	- 方差很低：它们在不同的随机数据样本上可以得出相似的结果。
- 深度神经网络：不局限于单独查看每个特征，而是学习**特征之间的交互**
- 
#### 计算图与传播

#### 数值稳定性和模型初始化

#### 环境和分布偏移

### 深度学习计算
### 卷积神经网络