> 吴恩达机器学习

## 监督学习
- 提供包含输入输出的数据集，算法通过学习数据集，实现对新输入生成输出
- 从有标签和正确答案的数据中学习
- 种类
	- 回归：预测一个具体值
	- 分类
### 线性回归
#### 成本函数
- 使用成本函数评估模型的表现
	- 通过这个函数评估模型预测结果的偏差大小
	- 这个值的大小不应该受到数据集的规模影响
- 平方差成本函数 $J(w,b) = \frac{1}{m}\sum^m_{i=1}(\hat{y}^{(i)}-y^{(i)})^2$
#### 梯度下降
- 一种计算最佳参数（成本函数代价最低）的方法
- $\alpha$ **学习率**表示“迈的步子的大小”
	- $\mathrm{w=w-\alpha}\frac \partial  {\partial w}J(w,b)$
	- $\mathrm{b=b-\alpha}\frac \partial  {\partial b}J(w,b)$
	- ![image.png|300](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250225171857.png)
	-  ![image.png|550](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250225182124.png)
	- 这个变化的过程就是梯度下降
	- ![image.png|550](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250225184807.png)
	- 与此同时 w 和 b 在逼近使得成本函数最小值的位置
- 学习率设置太小会导致梯度下降起作用速度太慢
- 学习率设置太大可能越过最优点来回震荡
- 可以通过设置较小的学习率来检测是否存在 bug（如果很小的学习率下仍然不收敛）
- 可以先设置一个较小的学习率，之后再逐渐尝试增大学习率，来实现选择尽可能大的学习率
- 
- ![image.png|600](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250225185645.png)
##### 检查梯度下降是否收敛
- 随着迭代次数增加，成本函数的值减少的越来越慢
	- ![image.png|500](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226114043.png)
- 除了以迭代次数作为算法结束条件，也可以使用自动收敛测试

##### 学习率的选择

#### 多元线性回归
- 从多个指标来预测结果
	- 即输入一个向量 $f_{\overrightarrow{\mathrm{w}},b}(\vec{\mathrm{x}})=\overrightarrow{\mathrm{w}}\cdot\vec{\mathrm{x}}+b$
- 与循环逐个计算相比，通常向量计算效率更高（因为可以更好的并行利用硬件加速）
##### 梯度下降用于多元线性回归
- 成本函数 $J(\overrightarrow{w},b)$
- $w_{j}=w_{j}-\alpha\frac{\partial}{\partial w_{j}}J(w_{1},\cdots,w_{n},b)$
- $b=b-\alpha\frac{\partial}{\partial b}J(w_{1},\cdots,w_{n},b)$
- ![image.png|500](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250225212535.png)
##### 功能缩放
- 多元线性回归中不同输入参数的大小范围可能差距很大，为了便于展示，可以对数据进行转换（缩放）
	- 梯度下降算法也可以找到通往最小值的**更直接的路径**（速度更快）
- ![image.png|450](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226112759.png)
- 即先对数据进行归一化
#### 多项式回归
- 有时需要根据输入与输出的关系，对输入的数据进行处理
	- 比如结果与输入的平方相关，那么就应该将数据数据改为其平方
	- 如输入房子的宽度和长度，而房价与面积相关，就应该将宽度和长度相乘
- 拟合一条多项式曲线
- $f_{\vec{w},b}(x)=w_{1}x+w_{2}x^{2}+w_{3}x^{3}+b$
### 逻辑回归
- 用于分类算法
- **sigmoid 函数**：将任意大小的数映射到 $0\sim1$ 范围
	- ![image.png|400](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226181003.png)
	- $g(z) = \frac{1}{1+e^{-z}}$
- 逻辑回归方程就是 $f_{\overrightarrow{\mathrm{W}},b}(\vec{\mathrm{x}})=g(\overrightarrow{\mathrm{w}}\cdot\vec{\mathrm{x}}+b)=\frac1{1+e^{-(\overrightarrow{\mathrm{w}}\cdot\overrightarrow{\mathrm{x}}+b)}}$
	- 计算结果表示有多大的概率为阳性

- 决策边界：以多少为边界，用于判断结果为真还是假（如 0.5）
#### 成本函数
-  不能沿用线性回归的成本函数，否则不能保证存在唯一极值点
	- ![image.png|296](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226194631.png)
- ![image.png|600](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226195240.png)
 - 预测正确时损失趋近于零。
 - 预测错误时损失迅速增加，迫使模型快速调整参数。
 - ![image.png|500](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226200559.png)
- 由此得到了一个凸函数，可以用于梯度求解
	- $J(\overrightarrow{\mathrm{w}},b)=\frac{1}{m}\sum_{i=1}^{m}L\left(f_{\overrightarrow{\mathrm{w}},b}\left(\vec{\mathrm{x}}^{(i)}\right),y^{(i)}\right)$
- 统一表示：$L\left(f_{\overrightarrow{w},b}\left(\vec{x}^{(i)}\right),y^{(i)}\right)=-\:y^{(i)}\mathrm{log}\left(f_{\overrightarrow{w},b}\left(\vec{x}^{(i)}\right)\right)-\left(1-y^{(i)}\right)\mathrm{log}\left(1-f_{\overrightarrow{w},b}\left(\vec{x}^{(i)}\right)\right)$
#### 过拟合
- 具有较大偏差时称为拟合不足
	- ![image.png|255](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226203424.png)
- 良好的拟合，可以很好的预测
	- 既没有高偏差也没有高方差
	- ![image.png|274](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226203631.png)
- 过拟合，对训练数据的拟合很好，但部剧本良好的预测能力
	- 通常具有很高的方差
	- ![image.png|303](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226203643.png)
- 解决过拟合
	- 使用更大的训练集
	- 使用更少的输入特征
	- 正则化，降低 $w_{i}$ 参数的系数，减少维度（x 幂的数目）
##### 正则化
- 正则化成本函数
	- $J(\vec{\mathbf{w}},b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{\mathbf{w}},b}(\vec{\mathbf{x}}^{(i)})-y^{(i)})^{2}+\frac{\lambda}{2m}\sum_{j=1}^{n}\omega_{j}^{2}$
	- "惩罚"所有的 $w_{i}$，加入到成本函数使得参数趋向于更小的值

- 正则化逻辑回归
	- 同样添加惩罚项 $\frac{\lambda}{2m}\sum_{j=1}^{n}w_{j}^{2}$
## 神经网络
### 神经网络模型
- 多层结构，每层从上一层获取输入向量，进行一系列处理转换后输出到下一层
	- 训练模型就是设置处理转换参数的过程
- ![image.png|600](https://thdlrt.oss-cn-beijing.aliyuncs.com/undefined20250226233450.png)
	- $a^{[l]}_{j} = g(\overrightarrow{w}^{[l]}_{j}\cdot \overrightarrow{a}^{[l-1]}+b^{[l]}_{j})$

## 无监督学习
- 从没有标签的数据中寻找特征
- 种类
	- 聚类算法：根据数据的特征对数据进行分组
	- 异常检测
## 推荐系统

## 强化学习